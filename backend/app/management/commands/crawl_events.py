import os
from typing import List
from django.core.management.base import BaseCommand
import dotenv

from app.services.event_processor import EventProcessor
from app.services.event_crawler import EventCrawler
from app.utils.url_utils import URLUtils


class Command(BaseCommand):
    help = "Crawl and process swimming events. Can process a single event or crawl multiple events from a website."

    def add_arguments(self, parser):
        # Create a mutually exclusive group for the mode
        mode_group = parser.add_mutually_exclusive_group(required=True)
        mode_group.add_argument(
            "--event",
            nargs="+",
            type=str,
            help="Process a single event. Provide one or more URLs for the same event.",
        )
        mode_group.add_argument(
            "--crawl",
            type=str,
            help="Crawl multiple events from a website (e.g., https://oceanmanswim.com/events/)",
        )
        mode_group.add_argument(
            "--profile",
            type=str,
            help="Use a specific crawl profile (e.g., outdoorswimmer)",
        )
        mode_group.add_argument(
            "--discovered",
            type=str,
            help="Process events from a discovered_event_urls.json file generated by discover_event_urls command",
        )

        # Optional arguments
        parser.add_argument(
            "--limit",
            type=int,
            help="Limit the number of events to process (for debugging)",
        )
        parser.add_argument(
            "--dry-run",
            action="store_true",
            help="Perform crawling and processing without saving to the database",
        )
        parser.add_argument(
            "--custom-prompt",
            type=str,
            help="Provide a custom prompt for the ReactAgent (only works with --profile)",
        )

    def handle(self, *args, **options):
        dotenv.load_dotenv()
        api_key = os.environ["FIRECRAWL_API_KEY"]
        dry_run = options.get("dry_run", False)

        if dry_run:
            self.stdout.write(
                self.style.WARNING(
                    "Running in DRY RUN mode - no database changes will be made"
                )
            )

        processor = EventProcessor(
            firecrawl_api_key=api_key,
            stdout=self.stdout,
            stderr=self.stderr,
            dry_run=dry_run,
        )

        if options["event"]:
            # Process single event mode
            result = self._process_single_event(processor, options["event"])
            # Add a reminder about dry run mode if applicable
            if dry_run and result:
                self.stdout.write(
                    self.style.WARNING(
                        "No database changes were made. Run without --dry-run to save events to the database."
                    )
                )
        elif options["profile"]:
            # Profile-based crawling mode
            self._crawl_with_profile(
                processor, api_key, options["profile"], options=options
            )
        elif options["discovered"]:
            # Process events from discovered_event_urls.json file
            self._process_discovered_events(
                processor, options["discovered"], limit=options.get("limit")
            )
        else:
            # Crawl multiple events mode
            self._crawl_multiple_events(
                processor, api_key, options["crawl"], limit=options.get("limit")
            )

    def _process_single_event(self, processor: EventProcessor, urls: List[str]):
        """Process a single event from provided URLs"""
        self.stdout.write(f"Processing event URLs: {', '.join(urls)}")
        event = processor.process_event_urls(urls)

        if event:
            if processor.dry_run:
                self.stdout.write(
                    self.style.SUCCESS(
                        f"[DRY RUN] Successfully processed event: {event.name} (would be saved to database)"
                    )
                )
            else:
                self.stdout.write(
                    self.style.SUCCESS(
                        f"Successfully processed event: {event.name} (ID: {event.id})"
                    )
                )
            return True
        else:
            dry_run_prefix = "[DRY RUN] " if processor.dry_run else ""
            self.stdout.write(
                self.style.ERROR(f"{dry_run_prefix}Failed to process event.")
            )
            return False

    def _crawl_multiple_events(
        self, processor: EventProcessor, api_key: str, start_url: str, limit: int = None
    ):
        """Crawl and process multiple events from a website"""
        crawler = EventCrawler(
            firecrawl_api_key=api_key,
            stdout=self.stdout,
            stderr=self.stderr,
        )

        # Get event URLs
        self.stdout.write(f"Crawling events from {start_url}")
        event_url_sets = crawler.get_event_urls(start_url)

        if not event_url_sets:
            self.stdout.write(self.style.WARNING("No events found"))
            return

        # Filter out URLs that already exist in the database
        self.stdout.write("Filtering out URLs that already exist in the database...")
        event_url_sets = URLUtils.filter_existing_url_sets(
            event_url_sets, stdout=self.stdout, stderr=self.stderr
        )

        if not event_url_sets:
            self.stdout.write(
                self.style.WARNING("All events already exist in the database")
            )
            return

        # Apply limit if specified
        if limit and limit > 0:
            original_count = len(event_url_sets)
            event_url_sets = event_url_sets[:limit]
            self.stdout.write(
                self.style.WARNING(
                    f"Limiting to {limit} events (found {original_count} total)"
                )
            )

        self.stdout.write(
            self.style.SUCCESS(f"Processing {len(event_url_sets)} events")
        )

        # Process each event
        successful = 0
        failed = 0
        for i, urls in enumerate(event_url_sets, 1):
            self.stdout.write(f"Processing event {i}/{len(event_url_sets)}")
            try:
                if self._process_single_event(processor, urls):
                    successful += 1
                else:
                    failed += 1
            except Exception as e:
                self.stderr.write(
                    self.style.ERROR(f"Failed to process event: {str(e)}")
                )
                failed += 1

        # Summary
        dry_run_prefix = "[DRY RUN] " if processor.dry_run else ""
        self.stdout.write(
            self.style.SUCCESS(
                f"\n{dry_run_prefix}Finished processing events:\n"
                f"- Successful: {successful}\n"
                f"- Failed: {failed}\n"
                f"- Total: {len(event_url_sets)}"
            )
        )

        if processor.dry_run:
            self.stdout.write(
                self.style.WARNING(
                    "No database changes were made. Run without --dry-run to save events to the database."
                )
            )

    def _process_discovered_events(
        self, processor: EventProcessor, json_file: str, limit: int = None
    ):
        """Process events from a discovered_event_urls.json file"""
        import json
        from app.services.event_crawler import EventCrawler

        # Load the discovered URLs from the JSON file
        try:
            self.stdout.write(f"Loading discovered event URLs from {json_file}")
            with open(json_file, "r") as f:
                discovered_urls = json.load(f)
        except Exception as e:
            self.stderr.write(
                self.style.ERROR(f"Failed to load discovered URLs: {str(e)}")
            )
            return

        if not discovered_urls:
            self.stdout.write(self.style.WARNING("No discovered URLs found in file"))
            return

        self.stdout.write(
            self.style.SUCCESS(f"Found {len(discovered_urls)} discovered URLs")
        )

        # Apply limit if specified
        if limit and limit > 0:
            original_count = len(discovered_urls)
            discovered_urls = discovered_urls[:limit]
            self.stdout.write(
                self.style.WARNING(
                    f"Limiting to {limit} events (from {original_count} total)"
                )
            )

        # Create an EventCrawler for processing multiple event URLs
        # Get the API key from the environment (same as used for the processor)
        import os

        api_key = os.environ["FIRECRAWL_API_KEY"]
        crawler = EventCrawler(
            firecrawl_api_key=api_key,
            stdout=self.stdout,
            stderr=self.stderr,
        )

        # Process each discovered URL
        successful = 0
        failed = 0
        skipped = 0
        multiple_processed = 0

        for i, url_data in enumerate(discovered_urls, 1):
            url = url_data.get("url")
            if not url:
                self.stderr.write(
                    self.style.ERROR(f"URL data at index {i-1} is missing 'url' field")
                )
                failed += 1
                continue

            # Check event type
            event_type = url_data.get("event_type", "unknown")

            if event_type == "unknown":
                self.stdout.write(
                    self.style.WARNING(
                        f"Skipping event {i}/{len(discovered_urls)}: {url} (Type: {event_type})"
                    )
                )
                skipped += 1
                continue

            # Process based on event type
            if event_type == "single":
                # Process as a single event
                self.stdout.write(
                    f"Processing single event {i}/{len(discovered_urls)}: {url}"
                )
                try:
                    if self._process_single_event(processor, [url]):
                        successful += 1
                    else:
                        failed += 1
                except Exception as e:
                    self.stderr.write(
                        self.style.ERROR(f"Failed to process single event: {str(e)}")
                    )
                    failed += 1

            elif event_type == "multiple":
                # Process as a page with multiple events
                self.stdout.write(
                    f"Processing multiple events page {i}/{len(discovered_urls)}: {url}"
                )
                try:
                    # Use EventCrawler to extract all event URLs from the page
                    self.stdout.write(f"Crawling events from {url}")
                    event_url_sets = crawler.get_event_urls(url)

                    if not event_url_sets:
                        self.stdout.write(
                            self.style.WARNING(f"No events found on {url}")
                        )
                        skipped += 1
                        continue

                    self.stdout.write(
                        self.style.SUCCESS(
                            f"Found {len(event_url_sets)} events on {url}"
                        )
                    )

                    # Filter out URLs that already exist in the database
                    self.stdout.write(
                        "Filtering out URLs that already exist in the database..."
                    )
                    filtered_event_url_sets = URLUtils.filter_existing_url_sets(
                        event_url_sets, stdout=self.stdout, stderr=self.stderr
                    )

                    if not filtered_event_url_sets:
                        self.stdout.write(
                            self.style.WARNING(
                                f"All events from {url} already exist in the database"
                            )
                        )
                        skipped += 1
                        continue

                    self.stdout.write(
                        self.style.SUCCESS(
                            f"After filtering, {len(filtered_event_url_sets)} of {len(event_url_sets)} events from {url} remain to be processed"
                        )
                    )

                    event_url_sets = filtered_event_url_sets

                    # Process each extracted event
                    multiple_successful = 0
                    multiple_failed = 0

                    for j, urls in enumerate(event_url_sets, 1):
                        self.stdout.write(
                            f"Processing extracted event {j}/{len(event_url_sets)} from {url}"
                        )
                        try:
                            if self._process_single_event(processor, urls):
                                multiple_successful += 1
                                successful += 1
                            else:
                                multiple_failed += 1
                                failed += 1
                        except Exception as e:
                            self.stderr.write(
                                self.style.ERROR(
                                    f"Failed to process extracted event: {str(e)}"
                                )
                            )
                            multiple_failed += 1
                            failed += 1

                    # Summary for this multiple events page
                    self.stdout.write(
                        self.style.SUCCESS(
                            f"Finished processing events from {url}:\n"
                            f"- Successful: {multiple_successful}\n"
                            f"- Failed: {multiple_failed}\n"
                            f"- Total: {len(event_url_sets)}"
                        )
                    )
                    multiple_processed += 1

                except Exception as e:
                    self.stderr.write(
                        self.style.ERROR(
                            f"Failed to process multiple events page: {str(e)}"
                        )
                    )
                    failed += 1

        # Summary
        dry_run_prefix = "[DRY RUN] " if processor.dry_run else ""
        self.stdout.write(
            self.style.SUCCESS(
                f"\n{dry_run_prefix}Finished processing discovered events:\n"
                f"- Successful: {successful}\n"
                f"- Failed: {failed}\n"
                f"- Skipped (unknown type): {skipped}\n"
                f"- Multiple event pages processed: {multiple_processed}\n"
                f"- Total URLs processed: {len(discovered_urls)}"
            )
        )

        if processor.dry_run:
            self.stdout.write(
                self.style.WARNING(
                    "No database changes were made. Run without --dry-run to save events to the database."
                )
            )

    def _crawl_with_profile(
        self,
        processor: EventProcessor,
        api_key: str,
        profile_id: str,
        options: dict = None,
    ):
        """Crawl and process events using a specific profile"""
        from app.crawl_profiles.profiles import get_profile

        # Load the profile
        profile = get_profile(profile_id)
        if not profile:
            self.stderr.write(self.style.ERROR(f"Profile '{profile_id}' not found"))
            return

        # Get the start URL from the profile
        start_url = profile.get("start_url")
        if not start_url:
            self.stderr.write(
                self.style.ERROR(f"Profile '{profile_id}' does not contain a start_url")
            )
            return

        # Check if a custom prompt was provided
        custom_prompt = options.get("custom_prompt")
        if custom_prompt:
            # Add the custom prompt to the profile
            profile["custom_prompt"] = custom_prompt
            self.stdout.write(
                self.style.SUCCESS(f"Using custom prompt provided via command line")
            )

        # Create a crawler with the profile
        crawler = EventCrawler(
            firecrawl_api_key=api_key,
            stdout=self.stdout,
            stderr=self.stderr,
            profile=profile,  # Pass the profile to the crawler
        )

        # Get event URLs
        self.stdout.write(
            f"Crawling events from {start_url} using profile '{profile.get('name', profile_id)}'"
        )
        event_url_sets = crawler.get_event_urls(start_url)

        if not event_url_sets:
            self.stdout.write(self.style.WARNING("No events found"))
            return

        # Filter out URLs that already exist in the database
        self.stdout.write("Filtering out URLs that already exist in the database...")
        event_url_sets = URLUtils.filter_existing_url_sets(
            event_url_sets, stdout=self.stdout, stderr=self.stderr
        )

        if not event_url_sets:
            self.stdout.write(
                self.style.WARNING("All events already exist in the database")
            )
            return

        # Apply limit if specified
        limit = options.get("limit")
        if limit and limit > 0:
            original_count = len(event_url_sets)
            event_url_sets = event_url_sets[:limit]
            self.stdout.write(
                self.style.WARNING(
                    f"Limiting to {limit} events (found {original_count} total)"
                )
            )

        self.stdout.write(
            self.style.SUCCESS(f"Processing {len(event_url_sets)} events")
        )

        # Process each event
        successful = 0
        failed = 0
        for i, urls in enumerate(event_url_sets, 1):
            self.stdout.write(f"Processing event {i}/{len(event_url_sets)}")
            try:
                if self._process_single_event(processor, urls):
                    successful += 1
                else:
                    failed += 1
            except Exception as e:
                self.stderr.write(
                    self.style.ERROR(f"Failed to process event: {str(e)}")
                )
                failed += 1

        # Summary
        dry_run_prefix = "[DRY RUN] " if processor.dry_run else ""
        self.stdout.write(
            self.style.SUCCESS(
                f"\n{dry_run_prefix}Finished processing events:\n"
                f"- Successful: {successful}\n"
                f"- Failed: {failed}\n"
                f"- Total: {len(event_url_sets)}"
            )
        )

        if processor.dry_run:
            self.stdout.write(
                self.style.WARNING(
                    "No database changes were made. Run without --dry-run to save events to the database."
                )
            )
